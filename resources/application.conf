application {
    application.spark-streaming-app="Avro processing Spark Streaming Kafka Application"
    zookeeper_host = "127.0.0.1"
    kafka_offset_zookeeper_node = "/kafkaOffset"
    sparkbatchinterval = 15
    driver-options = "-XX:+UseG1GC -Xms4g -Xmx8g -Dlog4j.configuration=file:/usr/local/spark-1.5.1-bin-hadoop2.6/conf/log4j_RequestLogDriver.properties"
    executor-options = "-XX:+UseG1GC -Dlog4j.configuration=file:/usr/local/spark-1.5.1-bin-hadoop2.6/conf/log4j_RequestLogExecutor.properties"
    spark-streaming-app=["spark.cores.max=64","spark.serializer=org.apache.spark.serializer.KryoSerializer", "spark.driver.memory=10g", "spark.executor.memory=15g", "spark.storage.memoryFraction=0.5",  "spark.driver.allowMultipleContexts=true", "spark.streaming.blockInterval=400ms", "spark.streaming.concurrentJobs=4", "spark.mesos.coarse=true", "spark.streaming.unpersist=true", "spark.sql.tungsten.enabled=true", "spark.sql.codegen=false", "spark.sql.unsafe.enabled=false" , "spark.streaming.kafka.maxRatePerPartition=25",  "spark.streaming.stopGracefullyOnShutdown=true"]
    }